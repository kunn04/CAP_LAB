{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "iiZ0mu0M4SQ4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiZ0mu0M4SQ4"
      },
      "source": [
        "## Comprobar HW y SW instalado\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMKkmKLCh43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda00820-061a-4d06-a554-6e4ea4904097"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:           6\n",
            "    Model:                85\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             3\n",
            "    BogoMIPS:             4000.26\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
            "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
            "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
            "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
            "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
            "                          fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f a\n",
            "                          vx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveop\n",
            "                          t xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     1 MiB (1 instance)\n",
            "  L3:                     38.5 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
            "                          pgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BH\n",
            "                          I: Vulnerable (Syscall hardening enabled)\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re6wntSW4GpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6139d61e-49eb-4f75-86ec-6383fc4949ce"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttSpDHvptOVn",
        "outputId": "d3b7b8a3-1bcb-43fe-feb4-b5e40f9cce82"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 15 15:31:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzoggfoq4YiF"
      },
      "source": [
        "## Tutorial: Multiplicación de matrices en GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptación de los ejemplos disponibles en los siguientes enlaces:\n",
        "*  https://medium.com/@harsh20111997/cuda-programming-2d-matrix-multiplication-1aa774b8d703\n",
        "* https://www.quantstart.com/articles/Matrix-Matrix-Multiplication-on-the-GPU-with-Nvidia-CUDA/"
      ],
      "metadata": {
        "id": "UFzEOc1kDxUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir cuda"
      ],
      "metadata": {
        "id": "p8l5dW0-hJcY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_WYQ2UU4TiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c0d8c2-9035-4de1-9b18-087b4abe8525"
      },
      "source": [
        "%%writefile cuda/MatMult.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define N 3\n",
        "\n",
        "__global__ void matrix_mul(int *a, int *b, int *c, int n) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int sum = 0;\n",
        "    if (i < n && j < n) {\n",
        "        for (int k = 0; k < n; k++)\n",
        "            sum += a[i * n + k] * b[k * n + j];\n",
        "        c[i * n + j] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void printMat( int *mm, char *nombre, int n){\n",
        "   for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++)\n",
        "\t\t\t\t\tprintf(\" %c[%d][%d] = %d \",nombre[0] ,i, j, mm[i * n + j]);\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = N;\n",
        "    int *a, *b, *c;\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    int size = n * n * sizeof(int);\n",
        "\t\tchar matA[]= \"A\";\n",
        "\t\tchar matB[]= \"B\";\n",
        "\t\tchar matC[]= \"C\";\n",
        "\n",
        "    a = (int *)malloc(size);\n",
        "    b = (int *)malloc(size);\n",
        "    c = (int *)malloc(size);\n",
        "\n",
        "\t\t// Inicializacion de valores\n",
        "    for (int i = 0; i < n; i++)\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            a[i * n + j] = i + j;\n",
        "            b[i * n + j] = i * j;\n",
        "        }\n",
        "\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockSize(N, N);\n",
        "    dim3 gridSize((n + N - 1) / N, (n + N - 1) / N);\n",
        "    matrix_mul<<<gridSize, blockSize>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printMat( a, matA, n); printf(\"\\n\");\n",
        "    printMat( b, matB, n); printf(\"\\n\");\n",
        "\t\tprintMat( c, matC, n);\n",
        "\n",
        "\t\t/* for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++)\n",
        "\t\t\t\t\tprintf(\" c[ %d ][ %d ] = %d \", i, j, c[i * n + j]);\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\t\t*/\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda/MatMult.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda/MatMult.cu -o cuda/MatMult\n",
        "!ls -la cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX_maQqhiVVc",
        "outputId": "1f75664e-05c0-4365-ac51-5a81e7583e75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 964\n",
            "drwxr-xr-x 2 root root   4096 Oct 15 15:31 .\n",
            "drwxr-xr-x 1 root root   4096 Oct 15 15:31 ..\n",
            "-rwxr-xr-x 1 root root 971280 Oct 15 15:31 MatMult\n",
            "-rw-r--r-- 1 root root   1852 Oct 15 15:31 MatMult.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmyKUKpLipOW",
        "outputId": "5eb01aed-709e-4db9-b46d-af293c92e43f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A[0][0] = 0  A[0][1] = 1  A[0][2] = 2 \n",
            " A[1][0] = 1  A[1][1] = 2  A[1][2] = 3 \n",
            " A[2][0] = 2  A[2][1] = 3  A[2][2] = 4 \n",
            "\n",
            " B[0][0] = 0  B[0][1] = 0  B[0][2] = 0 \n",
            " B[1][0] = 0  B[1][1] = 1  B[1][2] = 2 \n",
            " B[2][0] = 0  B[2][1] = 2  B[2][2] = 4 \n",
            "\n",
            " C[0][0] = 0  C[0][1] = 5  C[0][2] = 10 \n",
            " C[1][0] = 0  C[1][1] = 8  C[1][2] = 16 \n",
            " C[2][0] = 0  C[2][1] = 11  C[2][2] = 22 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio T1:\n",
        "En el ejercicio anterior se utiliza un grid de bloques 2D y en cada bloque una distribución de hilos también 2D.\n",
        "* Realice una version 1D tanto para el lanzamiento de bloques, como para la distribución de hilos dentro del bloque.\n",
        "* ¿Que versión es más eficiente? Justifique los resultados.\n"
      ],
      "metadata": {
        "id": "i2pwuSAdmsS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la cuda/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKPX1JSQE9DD",
        "outputId": "9d712345-5b8c-4a39-b0f0-7ded51a7bd04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root 971280 Oct 15 15:31 cuda/MatMult\n",
            "-rw-r--r-- 1 root root   1852 Oct 15 15:31 cuda/MatMult.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof cuda/MatMult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKlmzacPkgEl",
        "outputId": "01cd78c8-3797-43bf-9fe1-c3878195d008"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==754== NVPROF is profiling process 754, command: cuda/MatMult\n",
            " A[0][0] = 0  A[0][1] = 1  A[0][2] = 2 \n",
            " A[1][0] = 1  A[1][1] = 2  A[1][2] = 3 \n",
            " A[2][0] = 2  A[2][1] = 3  A[2][2] = 4 \n",
            "\n",
            " B[0][0] = 0  B[0][1] = 0  B[0][2] = 0 \n",
            " B[1][0] = 0  B[1][1] = 1  B[1][2] = 2 \n",
            " B[2][0] = 0  B[2][1] = 2  B[2][2] = 4 \n",
            "\n",
            " C[0][0] = 0  C[0][1] = 5  C[0][2] = 10 \n",
            " C[1][0] = 0  C[1][1] = 8  C[1][2] = 16 \n",
            " C[2][0] = 0  C[2][1] = 11  C[2][2] = 22 \n",
            "==754== Profiling application: cuda/MatMult\n",
            "==754== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   51.56%  4.2240us         1  4.2240us  4.2240us  4.2240us  matrix_mul(int*, int*, int*, int)\n",
            "                   25.78%  2.1120us         1  2.1120us  2.1120us  2.1120us  [CUDA memcpy DtoH]\n",
            "                   22.66%  1.8560us         2     928ns     672ns  1.1840us  [CUDA memcpy HtoD]\n",
            "      API calls:   97.93%  103.20ms         3  34.400ms  5.3280us  103.18ms  cudaMalloc\n",
            "                    1.42%  1.4933ms         1  1.4933ms  1.4933ms  1.4933ms  cuDeviceGetName\n",
            "                    0.25%  262.21us         1  262.21us  262.21us  262.21us  cudaLaunchKernel\n",
            "                    0.18%  194.46us       114  1.7050us     263ns  71.602us  cuDeviceGetAttribute\n",
            "                    0.13%  138.54us         3  46.178us  5.8640us  121.10us  cudaFree\n",
            "                    0.07%  70.927us         3  23.642us  8.2070us  37.087us  cudaMemcpy\n",
            "                    0.01%  8.0960us         1  8.0960us  8.0960us  8.0960us  cuDeviceGetPCIBusId\n",
            "                    0.01%  6.6910us         1  6.6910us  6.6910us  6.6910us  cuDeviceTotalMem\n",
            "                    0.00%  2.2680us         3     756ns     336ns  1.5120us  cuDeviceGetCount\n",
            "                    0.00%  1.2470us         2     623ns     316ns     931ns  cuDeviceGet\n",
            "                    0.00%     527ns         1     527ns     527ns     527ns  cuModuleGetLoadingMode\n",
            "                    0.00%     426ns         1     426ns     426ns     426ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517eff1f-f9bd-449d-9012-8336ae668322",
        "id": "qpPptPl7UBaG"
      },
      "source": [
        "%%writefile cuda/MatMultF.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <sys/time.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void matrixMultiplicationKernel(float* A, float* B, float* C, int N) {\n",
        "\n",
        "    int ROW = blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int COL = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "\n",
        "    float tmpSum = 0;\n",
        "\n",
        "    if (ROW < N && COL < N) {\n",
        "        // each thread computes one element of the block sub-matrix\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            tmpSum += A[ROW * N + i] * B[i * N + COL];\n",
        "        }\n",
        "    }\n",
        "    C[ROW * N + COL] = tmpSum;\n",
        "}\n",
        "\n",
        "// programa principal\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "\n",
        "    if(argc!=2) {\n",
        "      printf(\"Ha olvidado el valor de N, NxN.\\n\");\n",
        "      exit(1);\n",
        "    }\n",
        "    int N = atoi(argv[1]);\n",
        "    printf(\"tamaño= %d x %d \\n\", N,N);\n",
        "\n",
        "    struct timeval fin,ini;\n",
        "    long int tiempoGPU,tiempoCPU;\n",
        "\n",
        "//    int N = 16;\n",
        "    float *h_a, *h_b, *h_c;\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    int size = N * N * sizeof(float);\n",
        "\n",
        "    // reserva de memoria en el host\n",
        "    h_a = (float *)malloc(size);\n",
        "    h_b = (float *)malloc(size);\n",
        "    h_c = (float *)malloc(size);\n",
        "\n",
        "\t\t// Inicializacion de valores en la CPU (host)\n",
        "    for (int i=0; i<N; i++){\n",
        "        for (int j=0; j<N; j++){\n",
        "            h_a[i*N+j] = sin(i);\n",
        "            h_b[i*N+j] = cos(j);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // reserva de memoria en la GPU\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    // copia de valores a la memoria de la GPU\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    // dimensionar el numero de hilos ajustandolo al tamaño de la matriz\n",
        "    dim3 threadsPerBlock(N, N);\n",
        "    dim3 blocksPerGrid(1, 1);\n",
        "        if (N*N > 1024){\n",
        "            threadsPerBlock.x = 32;\n",
        "            threadsPerBlock.y = 32;\n",
        "            blocksPerGrid.x = ceil(double(N)/double(threadsPerBlock.x));\n",
        "            blocksPerGrid.y = ceil(double(N)/double(threadsPerBlock.y));\n",
        "        }\n",
        "    printf(\"threadsPerBlock.x= %d  y threadsPerBlock.y= %d \\n\", threadsPerBlock.x,threadsPerBlock.y);\n",
        "    printf(\"blocksPerGrid.x= %d  y blocksPerGrid.y= %d \\n\", blocksPerGrid.x,blocksPerGrid.y);\n",
        "\n",
        "    gettimeofday(&ini,NULL);\n",
        "\n",
        "    // Llamada al Kernel a ejecutar en la GPU\n",
        "    matrixMultiplicationKernel<<<blocksPerGrid,threadsPerBlock>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    gettimeofday(&fin,NULL);\n",
        "\n",
        "    tiempoGPU=(fin.tv_sec*1000000+fin.tv_usec)-(ini.tv_sec*1000000+ini.tv_usec);\n",
        "    printf(\"tiempo GPU: %ld us. \\n\",tiempoGPU);\n",
        "\n",
        "   // copia de resultados a la memoria de la CPU\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float *cpu_C;\n",
        "    cpu_C = (float *)malloc(size);\n",
        "\n",
        "     gettimeofday(&ini,NULL);\n",
        "    // Now do the matrix multiplication on the CPU\n",
        "    float sum;\n",
        "    for (int row=0; row<N; row++){\n",
        "        for (int col=0; col<N; col++){\n",
        "            sum = 0.f;\n",
        "            for (int n=0; n<N; n++){\n",
        "                sum += h_a[row*N+n]*h_b[n*N+col];\n",
        "            }\n",
        "            cpu_C[row*N+col] = sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    gettimeofday(&fin,NULL);\n",
        "    tiempoCPU=(fin.tv_sec*1000000+fin.tv_usec)-(ini.tv_sec*1000000+ini.tv_usec);\n",
        "    printf(\"tiempo CPU: %ld us. \\n\",tiempoCPU);\n",
        "\n",
        "\n",
        "    double err = 0;\n",
        "    // Check the result and make sure it is correct\n",
        "    for (int ROW=0; ROW < N; ROW++){\n",
        "        for (int COL=0; COL < N; COL++){\n",
        "            err += cpu_C[ROW * N + COL] - h_c[ROW * N + COL];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf (\"Error = %f \", err);printf(\"\\n\");\n",
        "\n",
        "// liberar memoria\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda/MatMultF.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc cuda/MatMultF.cu -o cuda/MatMultF\n",
        "!ls -la cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjUeKHxGg9ip",
        "outputId": "faac9092-7abe-410f-9206-5c42d5e836ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1920\n",
            "drwxr-xr-x 2 root root   4096 Oct 15 15:31 .\n",
            "drwxr-xr-x 1 root root   4096 Oct 15 15:31 ..\n",
            "-rwxr-xr-x 1 root root 971280 Oct 15 15:31 MatMult\n",
            "-rw-r--r-- 1 root root   1852 Oct 15 15:31 MatMult.cu\n",
            "-rwxr-xr-x 1 root root 971728 Oct 15 15:31 MatMultF\n",
            "-rw-r--r-- 1 root root   3658 Oct 15 15:31 MatMultF.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMultF 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXDhhoXdh-G7",
        "outputId": "bbbc560c-fc8d-4fd3-cd96-a19a47ae59e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 32 x 32 \n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 1  y blocksPerGrid.y= 1 \n",
            "tiempo GPU: 73318 us. \n",
            "tiempo CPU: 171 us. \n",
            "Error = 0.000048 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para interpretar correctamente los resultados, es importante diferenciar la ejecución de la primera vez que se utiliza la GPU al resto veces, y descartar esa primera vez por inicialización de la GPU."
      ],
      "metadata": {
        "id": "qs07AunzF94b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprobar los resultados con diferentes valores de N (tamaño de matriz NxN)"
      ],
      "metadata": {
        "id": "nuLoVHBhIK46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMultF 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTV9NMtbF550",
        "outputId": "49430f04-6d77-4877-b81e-987fbca2cd59"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 16 x 16 \n",
            "threadsPerBlock.x= 16  y threadsPerBlock.y= 16 \n",
            "blocksPerGrid.x= 1  y blocksPerGrid.y= 1 \n",
            "tiempo GPU: 253 us. \n",
            "tiempo CPU: 24 us. \n",
            "Error = -0.000011 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof cuda/MatMultF 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j5oJfDktfop",
        "outputId": "b536139d-0a86-4c36-d39f-082153c3809b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 16 x 16 \n",
            "==817== NVPROF is profiling process 817, command: cuda/MatMultF 16\n",
            "threadsPerBlock.x= 16  y threadsPerBlock.y= 16 \n",
            "blocksPerGrid.x= 1  y blocksPerGrid.y= 1 \n",
            "tiempo GPU: 246 us. \n",
            "tiempo CPU: 20 us. \n",
            "Error = -0.000011 \n",
            "==817== Profiling application: cuda/MatMultF 16\n",
            "==817== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   51.73%  4.7670us         1  4.7670us  4.7670us  4.7670us  matrixMultiplicationKernel(float*, float*, float*, int)\n",
            "                   26.74%  2.4640us         1  2.4640us  2.4640us  2.4640us  [CUDA memcpy DtoH]\n",
            "                   21.53%  1.9840us         2     992ns     768ns  1.2160us  [CUDA memcpy HtoD]\n",
            "      API calls:   99.39%  107.52ms         3  35.840ms  3.3730us  107.51ms  cudaMalloc\n",
            "                    0.22%  236.44us         1  236.44us  236.44us  236.44us  cudaLaunchKernel\n",
            "                    0.18%  189.76us       114  1.6640us     188ns  74.459us  cuDeviceGetAttribute\n",
            "                    0.13%  135.85us         3  45.284us  4.1090us  121.39us  cudaFree\n",
            "                    0.06%  60.902us         3  20.300us  8.1950us  26.377us  cudaMemcpy\n",
            "                    0.01%  13.109us         1  13.109us  13.109us  13.109us  cuDeviceGetName\n",
            "                    0.01%  8.1400us         2  4.0700us  2.5510us  5.5890us  cudaDeviceSynchronize\n",
            "                    0.01%  7.4810us         1  7.4810us  7.4810us  7.4810us  cuDeviceGetPCIBusId\n",
            "                    0.01%  6.4770us         1  6.4770us  6.4770us  6.4770us  cuDeviceTotalMem\n",
            "                    0.00%  1.8310us         3     610ns     282ns  1.0940us  cuDeviceGetCount\n",
            "                    0.00%  1.2280us         2     614ns     283ns     945ns  cuDeviceGet\n",
            "                    0.00%     656ns         1     656ns     656ns     656ns  cuModuleGetLoadingMode\n",
            "                    0.00%     508ns         1     508ns     508ns     508ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMultF 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSzAjfYHL0OL",
        "outputId": "33082d3d-6730-4030-d10c-8f84d9189d99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 1024 x 1024 \n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 32  y blocksPerGrid.y= 32 \n",
            "tiempo GPU: 6879 us. \n",
            "tiempo CPU: 13145171 us. \n",
            "Error = -0.116583 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof cuda/MatMultF 1024"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNMdYFJEL4LK",
        "outputId": "461c0a4f-5abf-475d-9fb3-01986aa26d46"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 1024 x 1024 \n",
            "==887== NVPROF is profiling process 887, command: cuda/MatMultF 1024\n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 32  y blocksPerGrid.y= 32 \n",
            "tiempo GPU: 6874 us. \n",
            "tiempo CPU: 8445860 us. \n",
            "Error = -0.116583 \n",
            "==887== Profiling application: cuda/MatMultF 1024\n",
            "==887== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.71%  6.6583ms         1  6.6583ms  6.6583ms  6.6583ms  matrixMultiplicationKernel(float*, float*, float*, int)\n",
            "                   18.65%  1.8898ms         1  1.8898ms  1.8898ms  1.8898ms  [CUDA memcpy DtoH]\n",
            "                   15.64%  1.5846ms         2  792.32us  787.12us  797.52us  [CUDA memcpy HtoD]\n",
            "      API calls:   86.01%  80.850ms         3  26.950ms  67.757us  80.705ms  cudaMalloc\n",
            "                    7.10%  6.6710ms         2  3.3355ms  5.1240us  6.6658ms  cudaDeviceSynchronize\n",
            "                    5.81%  5.4579ms         3  1.8193ms  979.12us  3.4574ms  cudaMemcpy\n",
            "                    0.71%  665.20us         3  221.73us  215.89us  225.15us  cudaFree\n",
            "                    0.21%  200.89us         1  200.89us  200.89us  200.89us  cudaLaunchKernel\n",
            "                    0.14%  131.85us       114  1.1560us     147ns  49.505us  cuDeviceGetAttribute\n",
            "                    0.01%  13.296us         1  13.296us  13.296us  13.296us  cuDeviceGetName\n",
            "                    0.01%  5.6070us         1  5.6070us  5.6070us  5.6070us  cuDeviceTotalMem\n",
            "                    0.01%  5.4560us         1  5.4560us  5.4560us  5.4560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.5560us         3     518ns     194ns     940ns  cuDeviceGetCount\n",
            "                    0.00%  1.3540us         2     677ns     309ns  1.0450us  cuDeviceGet\n",
            "                    0.00%     453ns         1     453ns     453ns     453ns  cuModuleGetLoadingMode\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMultF 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svDm4e63L1xB",
        "outputId": "5398a3f7-2bb4-468e-ca07-232cf6496482"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 64 x 64 \n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 2  y blocksPerGrid.y= 2 \n",
            "tiempo GPU: 242 us. \n",
            "tiempo CPU: 1403 us. \n",
            "Error = 0.000739 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof cuda/MatMultF 64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHi1yUBhMIDo",
        "outputId": "d9d4d513-279e-4e22-f518-727049ce168c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 64 x 64 \n",
            "==939== NVPROF is profiling process 939, command: cuda/MatMultF 64\n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 2  y blocksPerGrid.y= 2 \n",
            "tiempo GPU: 261 us. \n",
            "tiempo CPU: 1391 us. \n",
            "Error = 0.000739 \n",
            "==939== Profiling application: cuda/MatMultF 64\n",
            "==939== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.89%  19.040us         1  19.040us  19.040us  19.040us  matrixMultiplicationKernel(float*, float*, float*, int)\n",
            "                   22.04%  6.3680us         2  3.1840us  3.1680us  3.2000us  [CUDA memcpy HtoD]\n",
            "                   12.07%  3.4880us         1  3.4880us  3.4880us  3.4880us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.23%  99.003ms         3  33.001ms  4.5410us  98.981ms  cudaMalloc\n",
            "                    0.24%  242.49us       114  2.1270us     219ns  118.50us  cuDeviceGetAttribute\n",
            "                    0.23%  231.49us         1  231.49us  231.49us  231.49us  cudaLaunchKernel\n",
            "                    0.15%  147.94us         3  49.313us  4.7180us  126.44us  cudaFree\n",
            "                    0.09%  91.364us         3  30.454us  16.751us  40.057us  cudaMemcpy\n",
            "                    0.02%  22.934us         2  11.467us  3.0090us  19.925us  cudaDeviceSynchronize\n",
            "                    0.01%  11.863us         1  11.863us  11.863us  11.863us  cuDeviceGetName\n",
            "                    0.01%  7.5790us         1  7.5790us  7.5790us  7.5790us  cuDeviceGetPCIBusId\n",
            "                    0.01%  6.0380us         1  6.0380us  6.0380us  6.0380us  cuDeviceTotalMem\n",
            "                    0.00%  1.9180us         3     639ns     329ns  1.2450us  cuDeviceGetCount\n",
            "                    0.00%     994ns         2     497ns     289ns     705ns  cuDeviceGet\n",
            "                    0.00%     538ns         1     538ns     538ns     538ns  cuDeviceGetUuid\n",
            "                    0.00%     486ns         1     486ns     486ns     486ns  cuModuleGetLoadingMode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda/MatMultF 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah9GQclcL1M5",
        "outputId": "9c727227-5f27-4850-f5a4-f1f2679a5357"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 128 x 128 \n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 4  y blocksPerGrid.y= 4 \n",
            "tiempo GPU: 234 us. \n",
            "tiempo CPU: 7131 us. \n",
            "Error = 0.000598 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof cuda/MatMultF 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0fSVsCVMKUC",
        "outputId": "bcee3a5d-aa7c-4fd4-8e3f-4979d7ae20b8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamaño= 128 x 128 \n",
            "==957== NVPROF is profiling process 957, command: cuda/MatMultF 128\n",
            "threadsPerBlock.x= 32  y threadsPerBlock.y= 32 \n",
            "blocksPerGrid.x= 4  y blocksPerGrid.y= 4 \n",
            "tiempo GPU: 337 us. \n",
            "tiempo CPU: 12224 us. \n",
            "Error = 0.000598 \n",
            "==957== Profiling application: cuda/MatMultF 128\n",
            "==957== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.65%  34.367us         1  34.367us  34.367us  34.367us  matrixMultiplicationKernel(float*, float*, float*, int)\n",
            "                   30.81%  18.368us         2  9.1840us  8.8640us  9.5040us  [CUDA memcpy HtoD]\n",
            "                   11.54%  6.8800us         1  6.8800us  6.8800us  6.8800us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.28%  126.69ms         3  42.231ms  3.7880us  126.68ms  cudaMalloc\n",
            "                    0.23%  298.02us         1  298.02us  298.02us  298.02us  cudaLaunchKernel\n",
            "                    0.17%  215.76us       114  1.8920us     197ns  82.533us  cuDeviceGetAttribute\n",
            "                    0.14%  183.78us         3  61.261us  39.775us  91.888us  cudaMemcpy\n",
            "                    0.12%  154.45us         3  51.483us  5.1390us  131.15us  cudaFree\n",
            "                    0.03%  37.837us         2  18.918us  2.7980us  35.039us  cudaDeviceSynchronize\n",
            "                    0.01%  13.662us         1  13.662us  13.662us  13.662us  cuDeviceGetName\n",
            "                    0.01%  7.5400us         1  7.5400us  7.5400us  7.5400us  cuDeviceGetPCIBusId\n",
            "                    0.01%  6.4370us         1  6.4370us  6.4370us  6.4370us  cuDeviceTotalMem\n",
            "                    0.00%  2.0330us         3     677ns     311ns  1.3100us  cuDeviceGetCount\n",
            "                    0.00%  1.5100us         2     755ns     267ns  1.2430us  cuDeviceGet\n",
            "                    0.00%     775ns         1     775ns     775ns     775ns  cuModuleGetLoadingMode\n",
            "                    0.00%     424ns         1     424ns     424ns     424ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyefPtPn8HHu"
      },
      "source": [
        "### Ejercicio T2: Estudie el rendimiento del programa en función de N.\n",
        "\n",
        " Para guiarle en este estudio, se plantean a continuación diferentes cuestiones a responder:\n",
        "- ¿Qué valor de N obtiene Tgpu=Tcpu? ¿Qué pasa antes y después de este N? ¿Por qué?\n",
        "- ¿Cuál es el máximo valor de N que logra ejecutarse en la GPU? ¿Por qué?\n",
        "- ¿Qué coste extra en las operaciones de memoria tiene la GPU frente a la CPU? ¿Qué consideraría “justo” añadir para comparar con la CPU?\n"
      ]
    }
  ]
}